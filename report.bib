@COMMENT This file was generated by bib2html.pl <http://www.cs.cmu.edu/~pfr/misc_software/index.html#bib2html> version 0.90
@COMMENT written by Patrick Riley <http://www.cs.cmu.edu/~pfr>
@COMMENT This file came from Peter Stone's publication pages at
@COMMENT http://www.cs.utexas.edu/~pstone/papers
@Article{CSJ06,
	Author="Daniel Stronger and Peter Stone",
	title="Towards Autonomous Sensor and Actuator Model Induction on a Mobile Robot",
	journal="Connection Science",
	note="Special Issue on Developmental Robotics.",
	volume="18",number="2",year="2006",
	pages="97--119",
	abstract={
	          This article presents a novel methodology for a
	          robot to autonomously induce models of its actions
	          and sensors called ASAMI (Autonomous Sensor and
	          Actuator Model Induction).  While previous
	          approaches to model learning rely on an independent
	          source of training data, we show how a robot can
	          induce action and sensor models without any
	          well-calibrated feedback.  Specifically, the only
	          inputs to the ASAMI learning process are the data
	          the robot would naturally have access to: its raw
	          sensations and knowledge of its own action
	          selections.  From the perspective of developmental
	          robotics, our robot's goal is to obtain
	          self-consistent internal models, rather than to
	          perform any externally defined tasks.  Furthermore,
	          the target function of each model-learning process
	          comes from within the system, namely the most
	          current version of another internal system model.
	          Concretely realizing this model-learning methodology
	          presents a number of challenges, and we introduce a
	          broad class of settings in which solutions to these
	          challenges are presented.  ASAMI is fully
	          implemented and tested, and empirical results
	          validate our approach in a robotic testbed domain
	          using a Sony Aibo ERS-7 robot.
                 },
	wwwnote={<a href="http://www.tandf.co.uk/journals/titles/09540091.asp">Connection Science Journal</a>. Contains material that was previously published in an <a href="http://www.cs.utexas.edu/~pstone/Papers/2005icra/actsense.pdf">ICRA-2005 paper</a>.},
}

@MISC{And_learningand,
    author = {Sonia Chernova And and Abstract Legged Robots},
    title = {Learning and Using Models of Kicking Motions for Legged Robots},
    year = {}
}

@incollection(LNAI2007-ahmadi,
        author="Mazda Ahmadi and Peter Stone",
        title="Instance-Based Action Models for Fast Action Planning",
        booktitle= "{R}obo{C}up-2007: Robot Soccer World Cup {XI}",
        Editor="Ubbo Visser and Fernando Ribeiro and Takeshi Ohashi and Frank Dellaert",
        Publisher="Springer Verlag",address="Berlin",year="2008",
        series="Lecture Notes in Artificial Intelligence",      
	volume="5001",
	pages="1--16",
        abstract={
                Two main challenges of robot action planning in real domains
                are uncertain action effects and dynamic environments.
                In this paper, an instance-based action model is learned empirically
                by robots trying actions in the environment. Modeling the 
                action planning problem as a Markov decision process, 
                the action model is used to build the transition function. 
                In static environments,
                standard value iteration techniques are used for computing 
                the optimal policy.  In dynamic environments, an algorithm is proposed
                for fast replanning, which updates a subset of state-action values
                computed for the static environment. As a test-bed, the 
                goal scoring task in the RoboCup 4-legged scenario is used.
                The algorithms are validated in the problem of planning
                kicks for scoring goals in the presence of opponent robots.
                The experimental results both in simulation
                and on real robots show that the instance-based action model boosts
                performance over using parametric models as done previously, and also
                incremental replanning significantly improves over original off-line planning.
        },
        wwwnote={<b>BEST PAPER AWARD WINNER</b> at RoboCup International Symposium.<br>Official version from <a href="http://dx.doi.org/10.1007/978-3-540-68847-1_1">Publisher's Webpage</a>&copy Springer-Verlag},
)

@InProceedings{ICRA08-stronger,
	author="Daniel Stronger and Peter Stone",
	title="Maximum Likelihood Estimation of Sensor and Action Model Functions on a Mobile Robot",
	booktitle="{IEEE} International Conference on Robotics and Automation",
	location="Pasadena, CA",
	month="May",
	year="2008",
	abstract="In order for a mobile robot to accurately interpret its sensations and
		predict the effects of its actions, it must have accurate models of
		its sensors and actuators.  These models are typically tuned manually,
		a brittle and laborious process.  Autonomous model learning is a
		promising alternative to manual calibration, but previous work has
		assumed the presence of an accurate action or sensor model in order to
		train the other model.  This paper presents an adaptation of the
		Expectation-Maximization (EM) algorithm to enable a mobile robot to
		learn both its action and sensor model functions, starting without an
		accurate version of either.  The resulting algorithm is validated
		experimentally both on a Sony Aibo ERS-7 robot and in simulation.",
	wwwnote={<a href="http://www.icra2008.org/">ICRA 2008</a>},
 }

@article{oudeyer2006discovering,
  title={Discovering communication},
  author={Oudeyer, Pierre-Yves and Kaplan, Fr{\'e}d{\'e}ric},
  journal={Connection Science},
  volume={18},
  number={2},
  pages={189--206},
  year={2006},
  publisher={Taylor \& Francis}
}

@article{schmidhuber2006developmental,
  title={Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts},
  author={Schmidhuber, J{\"u}rgen},
  journal={Connection Science},
  volume={18},
  number={2},
  pages={173--187},
  year={2006},
  publisher={Taylor \& Francis}
}

@Article{IJAIT08-stronger,
	author="Daniel Stronger and Peter Stone",
	title="Polynomial Regression with Automated Degree: A Function Approximator for Autonomous Agents",
	volume = "17",
	journal = "International Journal on Artificial Intelligence Tools",
	number = "1",
	month = "February",
	year = "2008",
	pages = "159--174",
	abstract = 
                  "In order for an autonomous agent to behave robustly
                  in a variety of environments, it must have the
                  ability to learn approximations to many different
                  functions.  The function approximator used by such
                  an agent is subject to a number of constraints that
                  may not apply in a traditional supervised learning
                  setting.  Many different function approximators
                  exist and are appropriate for different
                  problems. This paper proposes a set of criteria for
                  function approximators for autonomous agents.
                  Additionally, for those problems on which polynomial
                  regression is a candidate technique, the paper
                  presents an enhancement that meets these criteria.
                  In particular, using polynomial regression typically
                  requires a manual choice of the polynomial's degree,
                  trading off between function accuracy and
                  computational and memory efficiency. Polynomial
                  Regression with Automated Degree (PRAD) is a novel
                  function approximation method that uses training
                  data to automatically identify an appropriate degree
                  for the polynomial. PRAD is fully
                  implemented. Empirical tests demonstrate its ability
                  to efficiently and accurately approximate both a
                  wide variety of synthetic functions and real-world
                  data gathered by a mobile robot.",
	wwwnote={<a href="http://www.worldscinet.com/ijait/17/1701/S02182130081701.html">
official published version</a>},
}

@inproceedings{ranasinghe2008surprise,
  title={Surprise-based learning for developmental robotics},
  author={Ranasinghe, Nadeesha and Shen, Wei-Min},
  booktitle={Learning and Adaptive Behaviors for Robotic Systems, 2008. LAB-RS'08. ECSIS Symposium on},
  pages={65--70},
  year={2008},
  organization={IEEE}
}

@COMMENT This file was generated by bib2html.pl <http://www.cs.cmu.edu/~pfr/misc_software/index.html#bib2html> version 0.90
@COMMENT written by Patrick Riley <http://www.cs.cmu.edu/~pfr>
@COMMENT This file came from Peter Stone's publication pages at
@COMMENT http://www.cs.utexas.edu/~pstone/papers
@InProceedings{ICDL10-hester,
	author="Todd Hester and Peter Stone",
	title="Real Time Targeted Exploration in Large Domains",
	booktitle = "The Ninth International Conference on Development and Learning (ICDL)",
	location = "Ann Arbor, Michigan",
	month = "August",
	year = "2010",
	abstract = "A developing agent needs to explore to learn about
		the world and learn good behaviors. In many real world tasks,
		this exploration can take far too long, and the agent must make
		decisions about which states to explore, and which states not
		to explore. Bayesian methods attempt to address this problem,
		but take too much computation time to run in reasonably sized
		domains. In this paper, we present TEXPLORE, the first algorithm
		to perform targeted exploration in real time in large domains.
		The algorithm learns multiple possible models of the domain
		that generalize action effects across states. We experiment with
		possible ways of adding intrinsic motivation to the agent to drive
		exploration. TEXPLORE is fully implemented and tested in a novel
		domain called Fuel World that is designed to reflect the type of
		targeted exploration needed in the real world. We show that
		our algorithm significantly outperforms representative examples
		of both model-free and model-based RL algorithms from the
		literature and is able to quickly learn to perform well in a large
		world in real-time.",
	wwwnote={<a href="http://www.eecs.umich.edu/icdl-2010/">ICDL 2010</a>},
}

@article{maes1993modeling,
  title={Modeling adaptive autonomous agents},
  author={Maes, Pattie},
  journal={Artificial life},
  volume={1},
  number={1\_2},
  pages={135--162},
  year={1993},
  publisher={MIT Press}
}

@book{motwani1995randomized,
  title={Randomized algorithms},
  author={Motwani, Rajeev},
  year={1995},
  publisher={Cambridge university press}
}
