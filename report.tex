\documentclass[10pt]{article}
\usepackage{latexsym}
\usepackage{qtree}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\title{Action Selection in ASAMI}
\author{Shun Zhang}
\date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

\sloppy
\section{Literature Review}

Authors of \cite{CSJ06} described a way that action model and sensor
model of a robot can be learned without external feedback.  In their
algorithm, called ASAMI, actions are selected randomly in the
training. However, this can be biased according to the current belief.
In this sense, the agent should be able to determine which action
would lead to most uncertain results and need more samples.  The agent
doesn't know the correctness of the models. It only knows the
consistency of them. For example, states with larger difference in
action model and sensor model ($|W_a - W_s|$) should be considered as
inconsistent. Unobserved states can be also assumed as inconsistent.

The first author of \cite{CSJ06}, Dan Stronger, commented that ``One
possible reason the $W_a$ is wrong after a certain action is that the
action is just very noisy \ldots This is a good reason to gather more
data for that action (the noisier something is, the more data you need
to estimate its expected value accurately).  But another possible
reason  an action could be causing problems is because the action
model function being fit, with the degrees of freedom that it has,
just fits to a function that's not especially accurate for that
action''. This would a problem in degree selection in the polynomial
regression, which is discussed in \cite{IJAIT08-stronger}. In this
paper, I'll give our action model proper degrees of freedom. So if the
action model is inconsistent, the reason should be either few data
gathered to make consistency happen, or the data gathered are noisy
and not making much sense.

There is a two-dimensional version of ASAMI discussed in
\cite{ICRA08-stronger}.  As the action space is larger in higher
dimension, biasing actions instead of random selection on actions
becomes more essential. This can be proceeded if this proposal works
in one-dimensional environment.

Compared with this proposed method, some similar ideas are discussed
in the developmental robotics literature \cite{oudeyer2006discovering,
schmidhuber2006developmental, ranasinghe2008surprise}. They call the
inconsistency described above as \textit{error}
\cite{oudeyer2006discovering} or \textit{surprise}
\cite{ranasinghe2008surprise}. This serves as a motivation to change
the model. \cite{oudeyer2006discovering} uses a reinforcement learning
framework and regards the error as reward. The authors in
\cite{ranasinghe2008surprise} let the robot figure out why there could
be a surprise. In this paper, I assume that the surprise is caused by
insufficient or noisy data.

There is an assumption in \cite{CSJ06} that there is a mapping from
action and difference of state, i.e. $A \rightarrow \Delta S$. This is
true in robot motion. If this is not true, so that $S \times A
\rightarrow S$ is the best we can estimate, this would become a more
interesting problem. We might know that a certain $(s, a)$ is
inconsistent and we want to gather more data on it. But our action
model might not not well-learned. So the reward should be a compromise
between inconsistency of that $(s, a)$ and the cost to reach it.

There are also other related works on learning the action model. But
they're using very different approach. They make use of the external
data and assume that the sensor model is well calibrated. The data are
represented as either statistics in \cite{And_learningand} or
instances in \cite{LNAI2007-ahmadi}.

%=====================================================================
\bibliographystyle{plain}

\bibliography{report}

\end{document}
