\documentclass[10pt]{article}
\usepackage{latexsym}
\usepackage{qtree}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\title{Action Selection in ASAMI}
\author{Shun Zhang}
\date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

\sloppy
\section{Literature Review}

The action model and sensor model of a robot can be learned
simultaneously without external feedback \cite{CSJ06}.  In the ASAMI
algorithm, actions are selected randomly in the training. However,
this can be biased according to the current belief.  In this sense,
the agent should be able to determine which action would lead to the
most uncertain results and thus need more samples.  The agent doesn't
know the correctness of the models. It only knows the consistency of
them. For example, states with larger difference in action model and
sensor model ($|W_a - W_s|$) should be considered as inconsistent.
Unobserved states can be also assumed as inconsistent.

In the perspective of model-adaptive agents \cite{maes1993modeling},
This solves the problem of \textit{action selection}, as it
accelerates the progress towards its goal - consistency between action
model and transition model. This also solves the problem of
\textit{learning from experience}, as the inconsistency is an
essential information that can be used for further decision-making.

The first author of \cite{CSJ06}, Dan Stronger, commented that ``one
possible reason the $W_a$ is wrong after a certain action is that the
action is just very noisy \ldots This is a good reason to gather more
data for that action \ldots  But another possible reason  an action
could be causing problems is because the action model function being
fit, with the degrees of freedom that it has, just fits to a function
that's not especially accurate for that action''. This would be a
problem in degree selection in the polynomial regression, which is
discussed in \cite{IJAIT08-stronger}. In this paper, I'll give our
action model proper degrees of freedom. So, if the action model is
inconsistent, the reason should be that either there are too few data
gathered to make consistency happen, or the data gathered are too
noisy.

There is a two-dimensional version of ASAMI discussed in
\cite{ICRA08-stronger}.  As the action space is larger in higher
dimension, biasing on actions instead of random selection on actions
becomes more essential. This can be proceeded if the proposed
modification works in one-dimensional environment.

Compared with this proposed method, some similar ideas are discussed
in the developmental robotics literature. They call the inconsistency
described above as \textit{error} \cite{oudeyer2006discovering},
\textit{surprise} \cite{ranasinghe2008surprise} or \textit{curiosity}
\cite{schmidhuber2006developmental}. This serves as a motivation to
change the model. A metric can be used to describe the global
inconsistency \cite{oudeyer2006discovering}. Then, a reinforcement
learning framework can be used to plan to get to the states with the
least global inconsistency. The authors in
\cite{ranasinghe2008surprise} used logic rules as the model, and let
the robot figure out why there could be a surprise. In this paper, I
assume that the surprise is caused by insufficient or noisy data. So
the surprise, or inconsistency, of an action, is simply caused by the
data gathered at that action.

Additionally, there is an assumption in \cite{CSJ06} that there is a
mapping from action to the difference of state, i.e., $A \rightarrow
\Delta S$.  This is true in robot motion. So it's possible to learn
the difference in the observation, when the action is given. This is
exactly how the task is designed in \cite{ICDL10-hester}. If this is
not true, so that $S \times A \rightarrow S$ is the best we can
estimate, this would become more challenging but still possibly to be
dealt with. We might know that a certain $(s, a)$ is inconsistent and
we want to gather more data on it. But our action model might not be
well-learned. So the reward should be a compromise between the
inconsistency of that $(s, a)$ and the cost to reach it.

There are also other related works on learning the action model. But
they're using very different approach. They assume that one model,
usually the sensor model, is well calibrated. The data are represented
as either statistics \cite{And_learningand} or instances
\cite{LNAI2007-ahmadi}.

%=====================================================================
\bibliographystyle{plain}

\bibliography{report}

\end{document}
