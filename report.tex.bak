\documentclass[10pt]{IEEEtran}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\title{Action Selection in ASAMI}
\author{Shun Zhang}
\date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

\section{Analysis of ASAMI}

The action model and sensor model of a robot can be learned
simultaneously without external feedback \cite{CSJ06}.  In the ASAMI
algorithm, actions are selected randomly in the training. However,
this can be biased according to the current belief.  In this sense,
the agent should be able to determine which action would lead to the
most uncertain results and thus need more samples.  The agent doesn't
know the correctness of the models. It only knows the consistency of
them. For example, states with larger difference in action model and
sensor model ($|W_a - W_s|$) should be considered as inconsistent.
Unobserved state, action pairs can be also assumed as inconsistent.

The first author of \cite{CSJ06}, Dan Stronger, commented that ``one
possible reason the $W_a$ is wrong after a certain action is that the
action is just very noisy \ldots This is a good reason to gather more
data for that action \ldots  But another possible reason  an action
could be causing problems is because the action model function being
fit, with the degrees of freedom that it has, just fits to a function
that's not especially accurate for that action''. This would be a
problem in degree selection in the polynomial regression
\cite{IJAIT08-stronger}. In this report, I'll give our action model
proper degrees of freedom. So, if the action model is inconsistent,
the reason should be that either there are too few data gathered to
make consistency happen, or the data gathered are too noisy.

\hfill

The action selection is even necessary in some environment. The
experiments in \cite{CSJ06} assume that \textit{we} know the effect
the actions -- for example, some actions would make it go forward, and
others make it go backward -- while the robot does not. So to explore
the environment, we make the robot walk forward and backward
alternatively to explore the state space. However, these the actions
may be totally unknown.  If the agent still chooses the actions uniformly
random, then exploration would be low effecient.

For example, we consider 1-dimensional walk. Let the world be in the
range of $[0,n]$. The agent starts at 0, and wants to reach $n$. Let
the range of velocity be $[-2a, 2a]$ (positive velocities mean going
forward, and negative ones mean going backward), and in every step the
agent chooses an action uniformly random.  Then, the expected absolute
velocity is $a$. Assume the agent would ``bump into'' the boundary of
$0$ and stays there if it tries to go backward from position $0$. The
expected \textit{moving forward} steps needed to take to reach
position $n$ from $0$ is $\frac{n}{a}$. This forms a \textit{random walk}
\cite{motwani1995randomized} problem. The expected steps needed to
take to reach distance $n$ is $O(n^2)$. This concludes that, if an
agent takes random actions in an unkown domain, it's likely that the
agent would restrict itself in a small sub-domain.

\section{Literature Review}

In the perspective of model-adaptive agents \cite{maes1993modeling},
this tries to solve the problem of \textit{action selection}, as it
accelerates the progress towards its goal - consistency between action
model and transition model. This also solves the problem of
\textit{learning from experience}, as the inconsistency is an
essential information that can be used for further decision-making.

There is a two-dimensional version of ASAMI discussed in
\cite{ICRA08-stronger}.  As the action space is larger in higher
dimension, biasing on actions instead of random selection on actions
becomes more essential. The proposed approach can be proceeded if the
proposed modification works in one-dimensional environment.

Compared with this proposed method, some similar ideas are discussed
in the developmental robotics literature. They call the inconsistency
described above as \textit{error} \cite{oudeyer2006discovering},
\textit{surprise} \cite{ranasinghe2008surprise} or \textit{curiosity}
\cite{schmidhuber2006developmental}. This serves as a motivation to
change the model. A metric can be used to describe the global
inconsistency \cite{oudeyer2006discovering}. Then, a reinforcement
learning framework can be used to plan to get to the states with the
least global inconsistency. The authors in
\cite{ranasinghe2008surprise} used logic rules as the model, and let
the robot figure out why there could be a surprise. In this report, I
assume that the surprise is caused by insufficient or noisy data. So
the surprise, or inconsistency, of an action, is simply caused by the
data gathered at that action.

Additionally, there is an assumption in \cite{CSJ06} that there is a
mapping from action to the difference of state, i.e., $A \rightarrow
\Delta S$.  This is true in robot motion. So it's possible to learn
the difference of the states, when an action is given. This is
exactly how the task is designed in \cite{ICDL10-hester}. If this is
not true, so that $S \times A \rightarrow S$ is the best we can
estimate, this would become more challenging but still possibly to be
dealt with. We might know that a certain $(s, a)$ is inconsistent and
we want to gather more data on it. But our action model might not be
well-learned. So the reward should be a compromise between the
inconsistency of that $(s, a)$ and the cost to reach it.

There are also other related works on learning the action model. But
they're using very different approach. They assume that one model,
usually the sensor model, is well calibrated. The data are represented
as either statistics \cite{And_learningand} or instances
\cite{LNAI2007-ahmadi}. These would be further analyzed and compared
with our approach in Section~\ref{sec:dis}.

\section{Proposed Algorithm}

\begin{algorithm*}
\caption{Strong ASAMI}\label{alg:asami}
\begin{algorithmic}[1]
\Function{initialize}{$rangeOfActions$}
    \State $trials\gets 0$
    \State $actions \gets$ discretified $rangeOfActions$
    \For{$action$ in $actions$}
        \State $samples[action] \gets \{\}$ \label{asa:actInit}
	\Comment{Initialize a sample set for each action}
        \State $s \gets A_0(action)$
	\State push $s$ to $samples[action]$
	\Comment{add the prediction by the initial action model}
    \EndFor
\EndFunction
\\
\Function{update}{$action, \Delta state$} \label{asa:update}
    \State add $\Delta state$ to $samples[action]$
    \State increase $trials$
\EndFunction
\\
\Function{getAction}{} \label{asa:getAct}
    \If {$trials\leq$ ACTION\_LEARNING\_TRIALS}
        \State $action \gets$ the action such that $samples[action]$ has the largest variance
    \Else
        \State $action \gets$ appropriate action for state exploration \label{asa:actSel2}
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm*}

In this section, I propose an algorithm, which I call it strong ASAMI,
that makes the agent choose action in a computationally cheap way.
This can be applied when there is no prior domain knowledge about
the action model. The idea is that we devide the learning process into two
phases. The agent learns the actions first and learns the state space
later. In the frist phase, it chooses the action with least confidence
to learn. In the second phase, it uses the actions it learned to
explore the environment.

Some essential functions are presented in Algorithm~\ref{alg:asami}.
In initialization, a sample list is initialized for each action
(Line~\ref{asa:actInit}). $A_0$ is the initial action model (a roughly
correct action model we have), and its prediction for each action is
put to the corresponding sample list.  This is a useful
initialization, as the variance for the sample list first represents
the error of our inital model.

In each iteration , update (Line~\ref{asa:update}) is called to update
the knowledge of the action effects. When the robot needs to decide which
is the next action to take, getAction (Line~\ref{asa:getAct}) is
called.

The action selection strategy in phase 2 (Line~\ref{asa:actSel2}) is
quite vague. However, this is up to the agent how to plan to traverse
the state space. For example, it might want to keep trying actions
with positive $\Delta state$. When it observes no state changes after
executing such action, then it tries actions with negative $\Delta
state$. Then the robot should have the performance of walking forward
and backward in the domain.

\section{Experiments}

The plan for the experimental evaluation is that, first, I test ASAMI
in a toy domain. I only naively implemented action model and sensor
model. Then, I made ASAMI and Strong ASAMI work on Nao.

Action model and sensor model are both assumed to be cubic functions.
We have four parameters to estimate for each model. The initial action
model $A_0$ is defined to be $A_0(c) = c$.

\subsection{Test in Simulator}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{simResult.png}
\caption{The Simulator.}
\label{fig:sim}
\end{figure}

The actions in the simulator are simply walking backward or foreward.
The sensor model computes the height of the beacon according to the
current state. As there is no noise in the simulator, the results show
that learning on both models converge quickly and nicely. The result
is in Figure~\ref{fig:sim}.

\subsection{Test on Nao: First Attempt}

The challenges when making ASAMI walk on Nao are that beacon height is
too noisy. The height returned by the beacon detector could be noisy,
as the observed image can be blur. The real walking velocity can be
different from what is set. The result is in Figure~\ref{fig:demo}.

One iteration is one frame on the Nao. Observation is retrieved and
ASAMI is updated in each iteration. It can be told from the beacon
height that the robot walks forward and backward two times, and then
kidnapped from a further point to a closer point to the beacon in
around 3,500 iteration.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{demoResult.png}
\caption{Testing of ASAMI on Nao during the class demo. The
observation comes continuously (30 frames per second). It's kidnapped
at around 3,500 step.}
\label{fig:demo}
\end{figure}

\subsection{Test on Nao: Second Attempt}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{out_obs.png}
\caption{Testing of ASAMI on Nao. The observation comes only when it's
in standing phase. So there is less update in the observation, and
less noise compared to Figre~\ref{fig:demo} (I re-scaled the height of
beacon, so that they can be plotted in a same magnitude).}
\label{fig:obs}
\end{figure}

I later noted that making Nao observe while walking is not necessary
(though interesting). So I made the Nao observe only when standing.
The performance is that, the Nao takes an action, and then walk, and
then stands still to look at the beacon. It keeps repeating this
sequence.  Also, the height of beacon in the figure is the average of
the observed beacon heights in the standing phase. The result is in
Figure~\ref{fig:obs}. Clearly, as there are less noise in the
observation, ASAMI has a better learning performance.

\subsection{Test on Nao: using Strong ASAMI}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{out_strong.png}
\caption{Testing of Strong ASAMI on Nao. The observation comes only
when it's in standing phase - same as Figure~\ref{fig:obs}. The robot
tries to learn the action model first in the first 1,500 iterations,
then continues to explore the state space (I re-scaled the height of
beacon, so that they can be plotted in a same magnitude).}
\label{fig:obs_strong}
\end{figure}

Strong ASAMI is applied in this experiment. The assumption is
different from the previous ones - Nao doesn't have the knowledge of
which action guarantees to make it walk forward or backward. The
results are in Figure~\ref{fig:obs_strong}. It can be told from the
beacon height that the robot tries different actions first (within
1,500 iterations) and then move forward and backward to explore the
state space.

Another intented goal is to show that covergence in
Figure~\ref{fig:obs_strong} should be faster than in previous
experiments. Actually, the difference is not significant, at least for
1-dimensional environment.

\section{Discussion and Conclusion}
\label{sec:dis}

It's possible that learning each discrete action independently would
have a better performance. Though it is interesting to estimate what
the most likely continuous model, expressed by polynomial function, by
the samples. It has several constraints. First, degree selection
should be determined, possibly using methods in
\cite{IJAIT08-stronger}. But the model many not be a small-dimensional
polynomial, so there might be too many parameters to estimate. Second,
the inherent drawback of function approximation method is that local
updates could have global effects. This is especially harmful in a
noisy environment.

The action model and sensor model are comparatively easy to learn for
a 1-dimenstional environment. So expecting improvement based on the
result of ASAMI algorithm maybe not realistic. However, Strong ASAMI
does show its power in the environment where action learning is
:q
necessary.

%=====================================================================
\bibliographystyle{plain}

\bibliography{report}

\end{document}
